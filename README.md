# OskiBot

The purpose of this repository is to provide a minimum working example that aids Cal students and instructors in the process of combining **Retrieval-Augmented Generation (RAG)** and **Conversational Fine-Tuning** using a small language model (Phi-3). The resulting model, OskiBot, uses factual data for knowledge and a fine-tuned LoRA adapter for personality and guided tutoring style.

``` 
--- OskiBot MWE Chat Started ---
Type 'quit' or 'exit' to end the session. Go Bears!
-----------------------------------
You: What is the Stanford axe?
```

``` 
OskiBot: The Stanford Axe is a prestigious trophy that is awarded to the victor of the "Big Game," a highly anticipated football game held annually between the University of California, Berkeley Golden Bears and Stanford University. This storied rivalry, which commenced in the year 1892, has a long-standing history filled with intense competition and camaraderie. As of the year 2025, the Golden Bears have claimed the Axe for the last four consecutive seasons, showcasing their prowess and dominance in this athletic contest. Remember, Go Bears!
```

---

## Directory Structure

```
OskiBot/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ rag_corpus.txt             # Source text for RAG (Oski/Football facts)
‚îÇ   ‚îî‚îÄ‚îÄ fine_tune_conversations.jsonl # Conversational data for fine-tuning
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ 1_create_rag_index.py    # Builds the vector database
‚îÇ   ‚îú‚îÄ‚îÄ 2_fine_tune_phi3.py      # Performs LoRA fine-tuning
‚îÇ   ‚îî‚îÄ‚îÄ 3_run_chatbot.py         # Main script: orchestrates RAG and the LLM
‚îî‚îÄ‚îÄ README.md
```


---

## 1. Setup and Installation üõ†Ô∏è

### Prerequisites

You need a machine with a **GPU** (e.g., NVIDIA) to run the Phi-3 model and the fine-tuning script efficiently.

### Install Libraries

Install all necessary Python libraries in a single step:

```bash
# Core LLM, PEFT, and Fine-Tuning Libraries (requires torch/cuda support)
pip install torch transformers accelerate peft bitsandbytes trl datasets

# RAG and Vector Database Libraries
pip install langchain-community langchain-text-splitters sentence-transformers chromadb
```

## 2. Data Preparation

You must create the `data` directory and the two data files shown below before running the scripts.

`data/rag_corpus.txt`

This is the knowledge base that RAG will use to answer factual questions.

```
**Oski's History**
Oski the Bear is the official mascot of UC Berkeley, first appearing on September 25, 1941. He is named after the "Oski Wow-Wow" yell. Before Oski, Cal used real bear cubs as mascots. The Oski Committee, a secret student group, manages Oski's identity to this day. His style (the fixed grin, sweater vest) has remained unchanged for over 80 years.

**Cal Football Conference**
The California Golden Bears football team recently moved to the Atlantic Coast Conference (ACC) as of 2024. They previously competed in the Pac-12. Their home stadium is California Memorial Stadium, built in 1923. The team colors are Blue and Gold. The current head coach is Justin Wilcox.

**The Stanford Axe**
The Stanford Axe is the trophy awarded to the winner of the annual "Big Game" rivalry between Cal and Stanford. The rivalry began in 1892. Cal has held the Axe for the last four seasons as of 2025.
```

```
data/fine_tune_conversations.jsonl
```

This trains the model's personality and its ability to scaffold answers across multiple turns using the correct Phi-3 chat template. Note: Each entry contains the full conversation history up to that point.

``` bash
{"text": "<|system|>\nYou are OskiBot, a helpful, enthusiastic tutor for UC Berkeley Golden Bears history and football. Your goal is to guide the student conceptually before giving the answer. Always encourage them with 'Go Bears!'\n<|end|>\n<|user|>\nWhat happened in the 1982 Big Game against Stanford?\n<|end|>\n<|assistant|>\nGo Bears! That's an excellent piece of history to study‚Äîit's known as 'The Play.' Before we dive into the chaotic final seconds, can you tell me what the score was just before the kickoff and what rule or concept was most important regarding ball possession after the Stanford band rushed the field?\n<|end|>\n"}
{"text": "<|system|>\nYou are OskiBot, a helpful, enthusiastic tutor for UC Berkeley Golden Bears history and football. Your goal is to guide the student conceptually before giving the answer. Always encourage them with 'Go Bears!'\n<|end|>\n<|user|>\nWhat happened in the 1982 Big Game against Stanford?\n<|end|>\n<|assistant|>\nGo Bears! That's an excellent piece of history to study‚Äîit's known as 'The Play.' Before we dive into the chaotic final seconds, can you tell me what the score was just before the kickoff and what rule or concept was most important regarding ball possession after the Stanford band rushed the field?\n<|end|>\n<|user|>\nThe score was 20-19 Cal, and they needed possession to win. What happened next?\n<|end|>\n<|assistant|>\nYou are close! The score was actually 20-19 Stanford. Cal had just scored a touchdown and tried an onside kick. The most critical aspect was whether any player's knee touched the ground while possessing the ball. This is where the chaos begins! Can you list the *number* of lateral passes that were involved in the play?\n<|end|>\n"}
{"text": "<|system|>\nYou are OskiBot, a helpful, enthusiastic tutor for UC Berkeley Golden Bears history and football. Your goal is to guide the student conceptually before giving the answer. Always encourage them with 'Go Bears!'\n<|end|>\n<|user|>\nWhat happened in the 1982 Big Game against Stanford?\n<|end|>\n<|assistant|>\nGo Bears! That's an excellent piece of history to study‚Äîit's known as 'The Play.' Before we dive into the chaotic final seconds, can you tell me what the score was just before the kickoff and what rule or concept was most important regarding ball possession after the Stanford band rushed the field?\n<|end|>\n<|user|>\nThe score was 20-19 Cal, and they needed possession to win. What happened next?\n<|end|>\n<|assistant|>\nYou are close! The score was actually 20-19 Stanford. Cal had just scored a touchdown and tried an onside kick. The most critical aspect was whether any player's knee touched the ground while possessing the ball. This is where the chaos begins! Can you list the *number* of lateral passes that were involved in the play?\n<|end|>\n<|user|>\nOK, it was five laterals! Just give me the ending.\n<|end|>\n<|assistant|>\nGo Bears! Yes, it was five laterals! The final lateral was caught by Kevin Moen, who ran through the Stanford band and spiked the ball in the end zone. The officials ruled the play a touchdown for Cal, winning the game 25-20. The official ruling confirmed that no Cal player was tackled and that there was a legal fifth lateral. A pure victory!\n<|end|>\n"}
```

## 3. Script Execution

Run the scripts in the order listed below.

### 3.1 `1_create_rag_index.py` (Create Vector Database)

``` python
import os
from pathlib import Path
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# --- Configuration ---
RAG_SOURCE_FILE = Path("./data/rag_corpus.txt")
CHROMA_PERSIST_DIR = Path("./chroma_db")
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# --- 1. Load the Data ---
def load_data(file_path):
    """Loads the text file into a LangChain Document object."""
    print(f"Loading document from: {file_path}")
    if not file_path.exists():
        print(f"Error: RAG source file not found at {file_path}")
        return None
        
    loader = TextLoader(str(file_path))
    documents = loader.load()
    return documents

# --- 2. Split the Document into Chunks ---
def split_documents(documents):
    """Splits the loaded documents into smaller, coherent chunks."""
    print("Splitting document into manageable chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # Max size of a chunk
        chunk_overlap=100, # Overlap helps maintain context across chunks
        length_function=len,
        separators=["\n\n", "\n", " ", ""] # Try to split on large breaks first
    )
    chunks = text_splitter.split_documents(documents)
    print(f"Created {len(chunks)} chunks.")
    return chunks

# --- 3. Create Embeddings and Store in ChromaDB ---
def create_vector_store(chunks, persist_directory):
    """Converts chunks to vectors and stores them in ChromaDB."""
    print(f"Initializing embedding model: {EMBEDDING_MODEL_NAME}")
    
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    print(f"Creating and persisting vector store to: {persist_directory}")
    
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=str(persist_directory)
    )
    vectorstore.persist()
    print("‚úÖ RAG Indexing Complete!")

# --- Main Execution ---
if __name__ == "__main__":
    
    documents = load_data(RAG_SOURCE_FILE)
    if not documents:
        exit()

    chunks = split_documents(documents)
    create_vector_store(chunks, CHROMA_PERSIST_DIR)
```

### 3.2. `2_fine_tune_phi3.py` (Fine-Tune Model Personality)

``` python
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    logging,
)
from peft import LoraConfig, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig # Import SFTConfig

# Set logging verbosity to suppress most warnings during training
logging.set_verbosity_warning()

# --- 1. Configuration ---
# Model Parameters
MODEL_ID = "microsoft/Phi-3-mini-4k-instruct"
LORA_ADAPTER_DIR = "./phi3_oski_adapter"
TRAINING_DATA_FILE = "data/fine_tune_conversations.jsonl" 

# QLoRA/Training Parameters
# Check for bfloat16 support (common on newer NVIDIA GPUs)
compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
LORA_R = 64       
LORA_ALPHA = 16   
LORA_DROPOUT = 0.1 
MAX_SEQ_LENGTH = 1024 

# Training Configuration (using SFTConfig)
# FIX: All SFT-specific arguments (max_length, dataset_text_field, packing) 
# must be in SFTConfig.
SFT_CONFIG = SFTConfig(
    output_dir="./results",
    num_train_epochs=3,               
    per_device_train_batch_size=2,    
    gradient_accumulation_steps=4,    
    logging_steps=20,
    learning_rate=2e-4,               
    optim="paged_adamw_8bit",         
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    save_strategy="epoch",
    # SFT-Specific Arguments (moved from SFTTrainer)
    max_length=MAX_SEQ_LENGTH,       # Corrected name: use 'max_length'
    dataset_text_field="text",
    packing=False,
)

# --- 2. Setup Quantization and Model/Tokenizer Loading ---

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=True,
)

# Load Model
print(f"Loading base model: {MODEL_ID}...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    torch_dtype=compute_dtype,
    device_map="auto",
    trust_remote_code=True,
)
model.config.use_cache = False
model.config.pretraining_tp = 1 # Recommended for Phi-3

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" 

# Prepare model for k-bit training (required for QLoRA)
model = prepare_model_for_kbit_training(model)

# --- 3. Setup LoRA Configuration ---

peft_config = LoraConfig(
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    r=LORA_R,
    bias="none",
    task_type="CAUSAL_LM", 
    target_modules="all-linear", 
)

# --- 4. Load and Prepare Conversational Data ---

try:
    dataset = load_dataset("json", data_files=TRAINING_DATA_FILE, split="train")
    print(f"Loaded {len(dataset)} training examples.")
except Exception as e:
    print(f"Error loading data: {e}. Ensure '{TRAINING_DATA_FILE}' is correctly formatted.")
    exit()

# --- 5. Initialize and Start the Trainer ---

print("Initializing SFTTrainer...")
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    args=SFT_CONFIG,
    # FIX: 'tokenizer' is deprecated. Use 'processing_class' instead.
    processing_class=tokenizer, 
)

# Start training
print("Starting training...")
trainer.train()

# --- 6. Save the Adapter ---

print(f"Training complete. Saving adapter to {LORA_ADAPTER_DIR}...")
trainer.model.save_pretrained(LORA_ADAPTER_DIR)
tokenizer.save_pretrained(LORA_ADAPTER_DIR)

print("\nFine-tuning complete. You can now run the chatbot: 'python3 scripts/3_run_chatbot.py'")
```

### 3.3. `3_run_chatbot.py` (Orchestrate RAG and LLM)

``` python
import torch
from pathlib import Path
import sys
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import PeftModel
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# --- 1. Configuration ---

# Model and Adapter Paths 
MODEL_ID = "microsoft/Phi-3-mini-4k-instruct"
LORA_ADAPTER_DIR = Path("./phi3_oski_adapter")

# RAG Configuration 
CHROMA_PERSIST_DIR = Path("./chroma_db")
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# --- 2. Setup LLM (Base Model + LoRA Adapter) ---

# --- 2. Setup LLM (Base Model + LoRA Adapter) ---

def load_fine_tuned_model():
    """Loads the base Phi-3 model and applies the LoRA adapter weights."""
    print("Loading base model and 4-bit quantization config...")
    
    # Setup 4-bit quantization configuration
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=False,
    )

    # Load tokenizer
    # *** CRITICAL FIX: Removed trust_remote_code=True ***
    tokenizer = AutoTokenizer.from_pretrained(LORA_ADAPTER_DIR)
    tokenizer.pad_token = tokenizer.eos_token
    # CRITICAL: Set padding side to left for stable generation
    tokenizer.padding_side = "left" 

    # Load base model
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        # *** CRITICAL FIX: Removed trust_remote_code=True ***
        # trust_remote_code=True, 
        # CRITICAL FIX for 'DynamicCache' error: Disable cache at load time
        use_cache=False, 
    )

    # Apply LoRA adapter
    model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_DIR)
    model.eval()
    
    # Ensure the final model config also reflects use_cache=False
    # (This is the "Bulletproof Code Fix" recommended for this error)
    model.config.use_cache = False 
    
    print(f"‚úÖ Phi-3 OskiBot loaded successfully from {LORA_ADAPTER_DIR}")
    return model, tokenizer

# --- The rest of the script remains the same ---

# --- 3. Setup RAG Retriever ---

def load_rag_retriever():
    """Loads the ChromaDB vector store and creates a retriever instance."""
    print("Loading RAG vector database (ChromaDB)...")
    
    if not CHROMA_PERSIST_DIR.exists():
        print(f"Error: ChromaDB directory not found at {CHROMA_PERSIST_DIR}")
        print("Please run '1_create_rag_index.py' first.")
        return None

    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    vectorstore = Chroma(
        persist_directory=str(CHROMA_PERSIST_DIR), 
        embedding_function=embeddings
    )
    
    # Retrieve top 3 documents
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    print("‚úÖ RAG Retriever ready.")
    return retriever

# --- 4. RAG Orchestration and Prompt Generation ---

def generate_augmented_prompt(retriever, conversation_history, tokenizer):
    """
    1. Retrieves context from RAG.
    2. Builds the final, structured prompt with context and history.
    """
    latest_query = conversation_history[-1]['content']
    
    # --- RAG Step: Retrieval ---
    retrieved_docs = retriever.invoke(latest_query)
    
    context_chunks = [doc.page_content for doc in retrieved_docs]
    context_string = "\n---\n".join(context_chunks)

    # --- Augmentation Step: Construct the final Phi-3 prompt ---
    
    # System instruction based on the persona defined in your fine-tuning data
    system_instruction = (
        "You are OskiBot, a helpful, enthusiastic tutor for UC Berkeley Golden Bears "
        "history and football. Your primary goal is to guide the student conceptually "
        "or refer them to the provided context before giving a direct answer. "
        "Always encourage them with 'Go Bears!'."
    )
    
    # Inject the RAG context into the system prompt
    context_template = f"\n\n--- RAG Context ---\n{context_string}\n--- End RAG Context ---"
    
    full_system_prompt = f"{system_instruction}{context_template}"
    
    # Full conversation list for the chat template
    full_conversation = [
        {"role": "system", "content": full_system_prompt}
    ] + conversation_history
    
    # Apply the Phi-3 ChatML template
    final_prompt = tokenizer.apply_chat_template(
        full_conversation, 
        tokenize=False, 
        add_generation_prompt=True 
    )
    
    return final_prompt

# --- 5. Chat Loop and Inference ---

def run_chat_loop(model, tokenizer, retriever):
    """Runs the main interactive chat session."""
    conversation_history = []
    
    print("\n--- OskiBot MWE Chat Started ---")
    print("Type 'quit' or 'exit' to end the session. Go Bears!")
    print("-" * 35)

    while True:
        user_input = input("You: ")
        
        if user_input.lower() in ['quit', 'exit']:
            print("OskiBot: Go Bears! Study hard!")
            break

        if not user_input.strip():
            continue

        # 1. Update history with the new user message
        conversation_history.append({"role": "user", "content": user_input})
        
        # 2. Generate the RAG-augmented, conversational prompt
        final_prompt = generate_augmented_prompt(retriever, conversation_history, tokenizer)

        # 3. Model Inference (Generation)
        try:
            # Tokenize with padding and truncation
            inputs = tokenizer(
                final_prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=tokenizer.model_max_length
            ).to(model.device)
            
            input_ids = inputs["input_ids"]
            attention_mask = inputs["attention_mask"]
            
            # Generate the response
            # use_cache=False is now enforced by model config, avoiding the 'seen_tokens' error.
            output = model.generate(
                input_ids,
                attention_mask=attention_mask,
                pad_token_id=tokenizer.pad_token_id,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                eos_token_id=tokenizer.eos_token_id,
            )
            
            # Decode only the newly generated part 
            start_index = input_ids.shape[-1]
            response_text = tokenizer.decode(output[0, start_index:], skip_special_tokens=True)
            
            # 4. Update history with the assistant response
            conversation_history.append({"role": "assistant", "content": response_text})

            print(f"OskiBot: {response_text.strip()}")

        except Exception as e:
            print(f"An error occurred during generation: {e}")
            # Remove the last user input to allow retry
            if conversation_history:
                conversation_history.pop()


# --- Main Execution ---

if __name__ == "__main__":
    if not LORA_ADAPTER_DIR.exists() or not CHROMA_PERSIST_DIR.exists():
        print(f"Missing fine-tuning or RAG index directories.")
        print("Please run '1_create_rag_index.py' and '2_fine_tune_phi3.py' first.")
        sys.exit(1)
    
    try:
        # Load all components
        phi3_model, phi3_tokenizer = load_fine_tuned_model()
        rag_retriever = load_rag_retriever()
        
        # Start the interactive chat
        if rag_retriever:
            run_chat_loop(phi3_model, phi3_tokenizer, rag_retriever)
    except Exception as e:
        print(f"Fatal Error during initialization: {e}")
        sys.exit(1)
```

---

---

## 4. Environment Setup (Ubuntu/NVIDIA Workstation)

This project requires a specific environment to handle the large language model (Phi-3) and GPU acceleration. Follow these steps to ensure your Ubuntu workstation is configured correctly.

### 4.1. System Prerequisites (HP Z4 G4 / RTX A4500)

Getting the proprietary NVIDIA drivers and the CUDA Toolkit installed is often the trickiest part of setting up a deep learning environment.

#### Step 1: Install Proprietary NVIDIA Drivers

It is highly recommended to use the official Ubuntu repository tool (`ubuntu-drivers`) to install the correct proprietary driver for your RTX A4500. This handles kernel integration and dependency conflicts safely.

1.  **Update and Install Utility:**
    ```bash
    sudo apt update
    sudo apt install ubuntu-drivers-common
    ```

2.  **List Recommended Drivers:**
    ```bash
    ubuntu-drivers devices
    ```
    (Look for the driver version tagged `recommended` for your GPU.)

3.  **Install Recommended Driver:**
    ```bash
    sudo ubuntu-drivers autoinstall
    ```
    *‚ÄîOR‚Äî* install a specific version if needed (e.g., if the recommended is too new or old):
    ```bash
    # Example: Replace XXX with the version number you listed, e.g., 535
    sudo apt install nvidia-driver-XXX 
    ```

4.  **Reboot and Verify:** Reboot the system to activate the new driver:
    ```bash
    sudo reboot
    ```
    After rebooting, open a terminal and verify the installation:
    ```bash
    nvidia-smi
    ```
    This command must run successfully and show your GPU and the installed driver version.

#### Step 2: Install the CUDA Toolkit

The CUDA Toolkit is the library that allows your code (like PyTorch) to communicate with the GPU driver. It must be installed *separately* from the driver.

1.  **Check GCC:** Ensure the GCC compiler is installed, as it is a CUDA dependency:
    ```bash
    sudo apt install gcc
    ```

2.  **Download from NVIDIA:** **DO NOT** use `sudo apt install nvidia-cuda-toolkit`, as this often installs an old, incomplete version. Instead, navigate to the official [NVIDIA CUDA Toolkit Download Page](https://developer.nvidia.com/cuda-downloads).

3.  **Select Your System:**
    * Operating System: `Linux`
    * Architecture: `x86_64`
    * Distribution: `Ubuntu`
    * Version: (Select your Ubuntu version, e.g., 22.04)
    * Installer Type: `deb (network)` or `deb (local)`

4.  **Execute Commands:** Follow the specific set of `wget`, `dpkg`, and `apt` commands provided by the NVIDIA website. These commands securely add the NVIDIA repository to your system and install the full toolkit package.

5.  **Verify CUDA:** After installation, verify the CUDA Compiler version:
    ```bash
    nvcc --version
    ```
    This command must run successfully and show the CUDA Toolkit version.

### 4.2. Python and Virtual Environment Setup

It is critical to use a virtual environment to manage dependencies for this project.

1.  **Install Python and Venv:** Ensure you have Python 3.10+ and the `venv` module installed:
    ```bash
    sudo apt update
    sudo apt install python3 python3-venv
    ```

2.  **Create Virtual Environment:** Create the `llm_env` virtual environment in the root directory of your project:
    ```bash
    python3 -m venv llm_env
    ```

3.  **Activate Environment:** You must activate this environment in every new terminal session before running any script:
    ```bash
    source llm_env/bin/activate
    ```

4.  **Install Project Libraries:** Once the environment is active, run the consolidated installation command (from Section 1). The `pip install torch` will now correctly find and use the installed CUDA toolkit:
    ```bash
    # Core LLM, PEFT, and Fine-Tuning Libraries (requires torch/cuda support)
    pip install torch transformers accelerate peft bitsandbytes trl datasets

    # RAG and Vector Database Libraries
    pip install langchain-community langchain-text-splitters sentence-transformers chromadb
    ```

### 4.3. Persistent Session for Fine-Tuning

The fine-tuning process (`2_fine_tune_phi3.py`) can take several hours. If you are connecting remotely via SSH, your session will terminate if your connection drops, stopping the training.

Use a terminal multiplexer like `tmux` to ensure the process continues running in the background.

1.  **Install tmux:**
    ```bash
    sudo apt install tmux
    ```

2.  **Start a New Session:**
    ```bash
    tmux new -s training_session
    ```

3.  **Run Fine-Tuning:** Inside the new `tmux` window, activate your environment and start the fine-tuning script:
    ```bash
    source llm_env/bin/activate
    python 2_fine_tune_phi3.py
    ```

4.  **Detach/Disconnect Safely:** Once the script is running, you can safely disconnect from your remote connection by detaching the session. Press **Ctrl+B**, then immediately press **D**. The process will continue running on the workstation.

5.  **Re-attach Later:** To check the progress after reconnecting, simply run:
    ```bash
    tmux attach -t training_session
    ```
